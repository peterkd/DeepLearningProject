{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cb4espuLKJiA"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Hub Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "execution": {
     "iopub.execute_input": "2020-11-26T02:26:24.929553Z",
     "iopub.status.busy": "2020-11-26T02:26:24.928897Z",
     "iopub.status.idle": "2020-11-26T02:26:24.930900Z",
     "shell.execute_reply": "2020-11-26T02:26:24.931318Z"
    },
    "id": "jM3hCI1UUzar"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_NEJlxKKjyI"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/classify_text_with_bert\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/classify_text_with_bert.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://tfhub.dev/google/collections/bert/1\"><img src=\"https://www.tensorflow.org/images/hub_logo_32px.png\" />See TF Hub model</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZ6SNYq_tVVC"
   },
   "source": [
    "# Classify text with BERT\n",
    "\n",
    "This tutorial contains complete code to fine-tune BERT to perform sentiment analysis on a dataset of plain-text IMDB movie reviews.\n",
    "In addition to training a model, you will learn how to preprocess text into an appropriate format.\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "- Load the IMDB dataset\n",
    "- Load a BERT model from TensorFlow Hub\n",
    "- Build your own model by combining BERT with a classifier\n",
    "- Train your own model, fine-tuning BERT as part of that\n",
    "- Save your model and use it to classify sentences\n",
    "\n",
    "If you're new to working with the IMDB dataset, please see [Basic text classification](https://www.tensorflow.org/tutorials/keras/text_classification) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PHBpLPuQdmK"
   },
   "source": [
    "## About BERT\n",
    "\n",
    "[BERT](https://arxiv.org/abs/1810.04805) and other Transformer encoder architectures have been wildly successful on a variety of tasks in NLP (natural language processing). They compute vector-space representations of natural language that are suitable for use in deep learning models. The BERT family of models uses the Transformer encoder architecture to process each token of input text in the full context of all tokens before and after, hence the name: Bidirectional Encoder Representations from Transformers. \n",
    "\n",
    "BERT models are usually pre-trained on a large corpus of text, then fine-tuned for specific tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCjmX4zTCkRK"
   },
   "source": [
    "## Setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:26:24.938620Z",
     "iopub.status.busy": "2020-11-26T02:26:24.935583Z",
     "iopub.status.idle": "2020-11-26T02:26:27.079198Z",
     "shell.execute_reply": "2020-11-26T02:26:27.079730Z"
    },
    "id": "q-YbjCkzw0yU"
   },
   "outputs": [],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q tensorflow-text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5w_XlxN1IsRJ"
   },
   "source": [
    "You will use the AdamW optimizer from [tensorflow/models](https://github.com/tensorflow/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:26:27.086726Z",
     "iopub.status.busy": "2020-11-26T02:26:27.084385Z",
     "iopub.status.idle": "2020-11-26T02:26:43.237684Z",
     "shell.execute_reply": "2020-11-26T02:26:43.237068Z"
    },
    "id": "b-P1ZOA0FkVJ"
   },
   "outputs": [],
   "source": [
    "!pip install -q tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:26:43.243455Z",
     "iopub.status.busy": "2020-11-26T02:26:43.242781Z",
     "iopub.status.idle": "2020-11-26T02:26:50.762490Z",
     "shell.execute_reply": "2020-11-26T02:26:50.762957Z"
    },
    "id": "_XgTpm9ZxoN9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optmizer\n",
    "from keras.datasets import imdb\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6MugfEgDRpY"
   },
   "source": [
    "## Sentiment Analysis\n",
    "\n",
    "This notebook trains a sentiment analysis model to classify movie reviews as *positive* or *negative*, based on the text of the review.\n",
    "\n",
    "You'll use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/) that contains the text of 50,000 movie reviews from the [Internet Movie Database](https://www.imdb.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vnvd4mrtPHHV"
   },
   "source": [
    "### Download the IMDB dataset\n",
    "\n",
    "Let's download and extract the dataset, then explore the directory structure.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:26:50.771667Z",
     "iopub.status.busy": "2020-11-26T02:26:50.770570Z",
     "iopub.status.idle": "2020-11-26T02:27:12.408748Z",
     "shell.execute_reply": "2020-11-26T02:27:12.408123Z"
    },
    "id": "pOdqCMoQDRJL"
   },
   "outputs": [],
   "source": [
    "#url = 'https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "#\n",
    "#dataset = tf.keras.utils.get_file('aclImdb_v1.tar.gz', url,\n",
    "#                                  untar=True, cache_dir='.',\n",
    "#                                  cache_subdir='')\n",
    "#\n",
    "#dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "#\n",
    "#train_dir = os.path.join(dataset_dir, 'train')\n",
    "#\n",
    "## remove unused folders to make it easier to load the data\n",
    "#remove_dir = os.path.join(train_dir, 'unsup')\n",
    "#shutil.rmtree(remove_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lN9lWCYfPo7b"
   },
   "source": [
    "Next, you will use the `text_dataset_from_directory` utility to create a labeled `tf.data.Dataset`.\n",
    "\n",
    "The IMDB dataset has already been divided into train and test, but it lacks a validation set. Let's create a validation set using an 80:20 split of the training data by using the `validation_split` argument below.\n",
    "\n",
    "Note:  When using the `validation_split` and `subset` arguments, make sure to either specify a random seed, or to pass `shuffle=False`, so that the validation and training splits have no overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:12.416891Z",
     "iopub.status.busy": "2020-11-26T02:27:12.416177Z",
     "iopub.status.idle": "2020-11-26T02:27:23.584962Z",
     "shell.execute_reply": "2020-11-26T02:27:23.584443Z"
    },
    "id": "6IwI_2bcIeX8"
   },
   "outputs": [],
   "source": [
    "#AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "#batch_size = 64\n",
    "#seed = 42\n",
    "#\n",
    "#raw_train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "#    'aclImdb/train',\n",
    "#    batch_size=batch_size,\n",
    "#    validation_split=0.2,\n",
    "#    subset='training',\n",
    "#    seed=seed)\n",
    "#\n",
    "#class_names = raw_train_ds.class_names\n",
    "#train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#\n",
    "#val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "#    'aclImdb/train',\n",
    "#    batch_size=batch_size,\n",
    "#    validation_split=0.2,\n",
    "#    subset='validation',\n",
    "#    seed=seed)\n",
    "#\n",
    "#val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#\n",
    "#test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "#    'aclImdb/test',\n",
    "#    batch_size=batch_size)\n",
    "#\n",
    "#test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGm10A5HRGXp"
   },
   "source": [
    "Let's take a look at a few reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download the IMDB dataset using TFDS. \n",
    "dataset, info = tfds.load('imdb_reviews', with_info=True,as_supervised=True)\n",
    "\n",
    "class_names = info.features['label'].names\n",
    "\n",
    "train_ds, test_ds = dataset['train'], dataset['test']\n",
    "\n",
    "vals_ds = tfds.load('imdb_reviews', split=[\n",
    "    f'train[{k}%:{k+10}%]' for k in range(0, 100, 10)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    version=1.0.0,\n",
       "    description='Large Movie Review Dataset.\n",
       "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=100000,\n",
       "    splits={\n",
       "        'test': 25000,\n",
       "        'train': 25000,\n",
       "        'unsupervised': 50000,\n",
       "    },\n",
       "    supervised_keys=('text', 'label'),\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 100\n",
    "BATCH_SIZE = 64\n",
    "train_ds = train_ds.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:23.590822Z",
     "iopub.status.busy": "2020-11-26T02:27:23.590143Z",
     "iopub.status.idle": "2020-11-26T02:27:23.618688Z",
     "shell.execute_reply": "2020-11-26T02:27:23.618048Z"
    },
    "id": "JuxDkcvVIoev"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [b\"There are people out there who will greenlight anything! That is the only explanation I can offer as to why the House of the Dead movie exists. And that's only scary part to the whole movie. It's so bad you'll go off movies forever. I seriously wanted to switch this off and turn the TV over to the Paint Drying channel but I was bound by my word to suffer the whole thing. I don't know why I do these bad things to myself.<br /><br />As if it matters, here's the basic jist of the 'story'. A group of twenty-somethings are so desperate to go out to some island in the Pacific Northwest (Canada actually, because it's cheap) for the 'Rave of the Century' (which consists of about 8 people and un-raving music) that they pay some craggy old fisherman $1000 to take them there after they miss the main ferry. That's gotta be some rave to be worth all that dough! The fisherman warns them that the island is also known as the Island of the Dead (hang on-I thought this was HOUSE of the Dead?) and that they are all doomed yadda yadda yadda.<br /><br />First faults here. Why would a tiny little rave (of the Century my foot!) be held on some remote island? Why would anyone willingly pay loads of money to get it? Why pay even more to the craggy old fisherman to take them back when they could just come back with the others?<br /><br />Once they arrive they discover that the rave (which consists of about 2 tents, a small stage and a port-a-john) has been smashed, there's blood everywhere and no one is around. What would any rationally thinking person do? Run for their lives of course. But no, these clueless, obviously blind people decide to go look for them. Soon enough they discover an old ramshackle house that's 50 times as big on the inside as it is on the outside. Another half hour of stumbling around in the forest follows, as an excuse to kill of some of the lesser characters, and after much tedium they arrive back at the house again. The characters, like the movie, go nowhere.<br /><br />Jammed into this ghastly disaster is a superabundance of gibberish dialogue, heinous acting, mumbo-jumbo exposition and zillions of clips from the once-popular arcade game of the same name. Why this was universally accepted as a good idea with the filmmakers I'll never know. The clips have no reference to any of the scenes and only degrade this trash even further, if that is at all possible.<br /><br />It has nothing to do with the game save for some cheap, throwaway line at the end. It makes Resident Evil look like cinematic glory. Hell, even the Double Dragon movie seems multi-Oscar worthy in comparison to this junk. The only one who comes out of this with his dignity still intact is Jurgen Prochnow. He could have just taken his money and ran but he tries his best with the awful script and brings a tiny bit of pathos to his character. The rest of the cast suck I'm afraid. The characters are idiots and deserve to die.<br /><br />Plus, if you cut out the swearing and pointless nudity, I see no reason why this film cannot be shown on Saturday morning TV. It's not frightening in the slightest. Pirates of the Caribbean is more scary than the skeletal bad guys in this film. And where did all those bad guys come from anyway? There were only a few people on the island to begin with. I guess this justifies the reason they chose to reuse footage over and over. I kid you not, you'll see the same zombie die a dozen times.<br /><br />Who's ultimately to blame for that scandalous waste of celluloid? None other than director Uwe Boll. His control over the movie is non-existent. You can clearly the see actors have no idea what they should be doing and that the zombies aren't really taking it all seriously. The actors seem like they're reading off cue cards as they constantly pause in the middle of long sentences and carry on talking as soon as they see the next card. It all feels very unnatural.<br /><br />Plus the film is shot like a two-part mini-series. I have indeed seen better TV productions. And don't get me started on the editing. The film is an incoherent babble with thousands upon thousands of pointless shots and dozens of meaningless camera pans. No real skill or talent was put into making this at all. It truly baffles and boggles the mind how movies this unfathomably bad can get made and George A. Romero can't even get anyone to take his calls. House of the Dead makes some idiotic reference to Romero in a lazy attempt to be 'post-modern' but it only irritates that they think THIS is in the same league as a REAL zombie movie.<br /><br />For what it's worth, the 1.85:1 anamorphic picture looks great and the Dolby 5.1 soundtrack is clean but very unimpressive and only serves to pronounce the heavily over-used ADR even more. The DVD comes with extras but why torture yourself. Isn't this review warning enough? Stay away! You are all doomed I tell you! Doomed! Doomed!!!\"\n",
      " b\"All of those who voted less than 5 are obviously not fans of clean, tongue-in-cheek humor. Keaton is brilliant in this - as in most of his work. This is not a blockbuster, bigger-than-life affair. This is campy, slapstick humor played out by some of Hollywood's best (and very versatile) actors. Piscopo was equally on the mark as the top dog wannabee, once.<br /><br />If you want to see the funniest attempt at not really cussing ever filmed, you gotta see Dimitri do his piece as Morone.<br /><br />I gave it a 7.\"\n",
      " b\"Average (and surprisingly tame) Fulci giallo which means it's still quite bad by normal standards, but redeemed by its solid build-up and some nice touches such as a neat time twist on the issues of visions and clairvoyance.<br /><br />The genre's well-known weaknesses are in full gear: banal dialogue, wooden acting, illogical plot points. And the finale goes on much too long, while the denouement proves to be a rather lame or shall I say: limp affair.<br /><br />Fulci's ironic handling of giallo norms is amusing, though. Yellow clues wherever you look.<br /><br />3 out of 10 limping killers\"\n",
      " b'The best Laurel and Hardy shorts are filled to the brim with mishaps, accidents and destruction, mostly caused by Stan, but with Ollie receiving the bulk (!) of the punishment-- see the great \\'The Music Box\\' (1933) or \\'Towed in a Hole\\' (1932) as some some classic examples.<br /><br />Here, however, for some reason (is it because it was based on a sketch by Stan\\'s father?) the boys play it \\'straight\\' in a \\'comedy\\' built around jokes and supposedly funny situations. It doesn\\'t come off. It\\'s merely another third-rate tedious 30s comedy, heightened only by the personalities of Stan and Ollie who never really display any of their trademarked gestures (Ollie\\'s finger wiggling, Stan\\'s blank stares, etc.) or comic abilities.<br /><br />The film begins with them running from the police. Since we never see or know why, it\\'s hard to believe or accept their fear of being caught, and thus hiding in Colonel Buckshot\\'s mansion. The premise for the \\'humor\\', Ollie passing himself off as the Colonel and Stan passing himself off as both the butler and the maid are never very engaging. They are not playing \\'Stan and Ollie\\' in this film. Their parts could have been played by any of the pedestrian studio actors and it would be just as poor.<br /><br />Stan could mime and make whatever he would do funny, but he doesn\\'t get the chance to do any of that here. He\\'s constrained by uttering too much dialog to \\'move\\' the plot, but none of it rises much above the silly. We are treated to endless third rate comedy chestnuts such as the running gag of not correctly pronouncing Lord Plumtree\\'s name, the \"Call me a cab! Okay you\\'re a cab!\" joke, cops losing their clothes and being seen in long johns, and a non-sequiter ending of Stan and Ollie as the two parts in a painfully obvious horse costume as they make their escape on a bicycle for two, and James Finlayson is still doing his silent-era full body takes and Keystone Kop jumping jacks.<br /><br />Stan and Ollie do much better in a situation comedy in \\'Sons of the Desert\\'(1933) where we get to see them do what we love about them -- be themselves. In fact, 1932-34 seem to be their best years.<br /><br />Since this film does not play to any of their strengths, why bother with it? I have to give it a 3.'\n",
      " b'Just given the fact that it is based on the most infamous mass suicide incident of modern times would have been enough to give this 2-part 1980 made-for-TV film attention. But the fact is that it is a superb recreation of the life of the Rev. Jim Jones, who built a church into a virtual empire, and then encouraged it to disintegrate into a sleazy cult in which a Congressman and his entourage were assassinated, and 917 cult followers committed suicide by drinking Kool-Aid doused with cyanide.<br /><br />Done very tastefully but horrifying enough, unlike the excruciatingly sadistic CULT OF THE DAMNED, GUYANA TRAGEDY features an all-star cast, including Ned Beatty (as Rep. Leo Ryan), Meg Foster, Randy Quaid, Brad Dourif, Brenda Vaccaro, LeVar Burton, and Madge Sinclair. But it is Powers Boothe (in his first big role) that really stands out as Jim Jones. He actually BECOMES the man, and his performance is riveting and chilling. Thus, it is no wonder that this film still manages to attract attention after more than twenty years.'\n",
      " b\"I have a six month old baby at home and time to time she fights sleep really bad. One morning she was having a particular difficult time getting to sleep when the doodle bops theme song came on T.V. She stopped crying almost instantly, and for the rest of the show was content. I sat her in her bouncy seat and watched her kick her legs, swing her arms, and actually laugh at this show. The kept her entertained and happy the entire time. I also got a video of them so that at times when my little one is flustered I have something to calm her. Granted, late at night if she awakes with colic to fuss the doodle bops are not her cup of tea, but they sure do come in handy when I need a little time to do housework,etc. The biggest surprise about the doodle bops is that my child doesn't even like watching T.V. She'd rather be in the floor playing with a toy or with our small toy poodle than watch T.V. yet, the doodle bops have totally captured her attention. I don't know if she will continue to like them in the future but for now she's attached.\"\n",
      " b\"I suppose all the inside jokes is what made Munchies a cult classic. I thought it was awful, though given the ridiculous story and the nature of the characters, it probably could've been a much better (and funnier) movie. Maybe all they needed was a real budget.<br /><br />Munchies, as many viewers have pointed out already, is something of a Gremlins parody. Hence, all the references to the movie. The movie begins somewhere in Peru during an archeological dig. An annoying dufus named Paul, aspiring stand up comedian who offers no sarcasm or witty jokes during the movie despite his career plans, is holed up with his dad in the caves. His dad is an unconventional kind of archeologist, searching the caves not for artificats or mummies or anything, but proof of U.F.O.'s. And that's where the Munchies come into the picture. Hidden in the crevice of a rock is an ugly little mutant that looks like a gyrating rubber doll with a Gizmo voice. They name him Arnold, stash him in a bag, and bring him home so Paul's dad can finally show proof of extra terrestrial life.<br /><br />Paul, the idiot that he is, breaks his promise to his dad to watch Arnold (a wager he made with his dad, if he loses, it's off to community college to get a 'real' career). The creepy next door neighbor with the bad rug, Cecil (television veteran Harvey Korman), wonders what his neighbors are up to. So, he and his lazy son, some airhead hippie type (who looks more like they should've made his character a biker or heavy metal enthusiast) to go and snatch Arnold. Why? A get rich quick scheme of course. And of course, even Cecil's son is too dumb to look after Arnold. And after a few pokes and prods at Arnold, he multiplies into more Munchies.<br /><br />This wasn't even a movie that was so bad it was good. It was just plain awful. I was hoping that the Munchies would've mutated and killed the morons that were always after them, even Paul and his girlfriend. At least it would be one way to get rid of all the bad acting in this movie that really hams up the movie. Not to mention poor special effects that look like hand puppets. And really bad writing all around--it wasn't even funny--not even that young cop who can really give you the homicidal twitch in your eye. Like I said, Munchies, if they had been given an actual budget and better actors, they might've been able to pull off a good parody. Pass.\"\n",
      " b'Complete waste of time.... This movie is not comedy, it\\'s not drama, it\\'s not romance...not even teenage comedy at least!!! Story... it should be some turn-over one end... but it\\'s so disappointing! When movie has a turn-over on end I expect that turn-over to make movie even better (exp. \"Fight Club\") but this turn-over makes movie even worse.... When I watch teenage comedy, and I don\\'t do that very often, I expect lousy jokes and bunch of nudeness... Jokes are too lousy and there is no nudity... You got only one....very good looking I must admit... girl, and that\\'s that! And she\\'s fully dressed whole movie! Acting is bad... like soap series... Don\\'t waste your time! There are porns with better story and acting!<br /><br />(sorry on my bad English)'\n",
      " b\"If this movie were more about Piper Perabo's character and less about the bar, this might have been halfway decent. Piper's Violet Stanford and Karen Friendly (Adventures of Rocky and Bullwinkle) both have a virgin kindegarden teacher quality to them that's endearing. Here's hoping she'll find a better movie to be in.\"\n",
      " b'(Review in English, since Swedish is not allowed)<br /><br />I saw this movie with extremely low expectations, and I can sadly inform you that the movie barely lived up to them.<br /><br />As much as I loved to see Janne \"Loffe\" Karlsson on the big screen again, the writers should have realized early in the scriptwriting process that seven people falling into the water, isn\\'t original or funny. The story is very thin and the jokes are used and predictable, the ones that ain\\'t, is just plain boring. I smiled like three times during the entire film.<br /><br />The placement of Swedish Findus products is (unintentionally) funny, why not just a big sign saying; \"Findus made it happen!\".<br /><br />G\\xc3\\xb6ta Kanal 2 doesn\\'t need to be seen at the cinema or on DVD, just wait for it to air on TV, it wont take too long.'\n",
      " b'Scary Movie 2 is definitely the worst of the 4 films, for there is not much of a plot , bad acting, pretty tedious and some really cheesy jokes. But. And this is a big but, there is one good actor, one good recurring joke, and a good beginning. The good actor being Tim Curry, the one good recurring joke is the creepy,weird butler with the disgusting hand who always does cringey but laugh worthy things. And the good beginning is the spoof of the Excorsist.<br /><br />The plot to Scary Movie 2 is the main characters from the original and a host of new characters along the way are invited to stay the night at a creepy old mansion, but will they survive the night? This film is not very good but if your bored you might as well watch it!'\n",
      " b'Aro Tolbukhin burnt alive seven people in a Mission in Guatemala in the 70\\'s. Also he declared that he had murdered another 16 people (he used to kill pregnant women, and then he set them on fire).<br /><br />This movie is a documentary that portraits the personality of Aro through several interviews with people that got to know him and through some scenes played by actors based on real facts.<br /><br />\"Aro Tolbukhin\" is a serious work, so analytical, it\\'s not morbid at all. Such a horrifying testimony about how some childhood trauma can turn a man into a monster.<br /><br />*My rate: 7/10'\n",
      " b\"I just finished watching this on TV and what can I say but this is the worst film I have EVER seen! I'm embarrassed to be from Melbourne, where the film was made. Diabolical acting, amateurish makeup effects and a REALLY bad soundtrack. As for the plot, well, thats even MORE stupid! Some of the scenes just left me stunned as to how bad it was. There's a reason they put these types of films on late night TV - because they're utter rubbish! Avoid at all costs.\"\n",
      " b'I recently had the pleasure of seeing The Big Bad Swim at the Ft. Lauderdale Film Festival and I must say it is the best film I have seen all year and the only film I have ever felt inspired to write a comment/review on. This film was beautifully directed and combined a script with realistic dialogs, excellent acting, and an inspiring message. Ordinary lives come together in an adult swim class and become extraordinary in a celebration of the diversity of life. This is poignantly illustrated by the imagery in the first minute of this captivating film where we see only the legs and torso of individuals in various shapes and sizes enter into a pool of water. This film is brilliantly directed as the actors are placed and positioned in captivating scenes, which hold your attention and imagination.'\n",
      " b\"Legendary director Sidney Lumet gives us one of his finest films in his historic career in this very tense, and ultimately shocking story about a family that includes dysfunctional as one of the children. With an A-list cast headed by Philip Seymour Hoffman (an Oscar-worthy performance here), Ethan Hawke, Marisa Tomei and Albert Finney, Lumet has captured not just elements of botched crime stories such as Reservoir Dogs, but also family stories such as Ordinary People.<br /><br />Many viewers might be confused and feel underwhelmed at the construction of the plot Lumet has gone with here. Instead of showing it in a linear manner, he has gone the Tarantino route and shows the central scene of a robbery gone wrong from different points of view all out of order. I personally found this to be very satisfying and left me constantly guessing what was going to happen next. The script is very strong with some excellent scenes between husband and wife Hoffman and Tomei, as well as between father and son Finney and Hoffman. All the actors are totally engaging to watch and Lumet is obviously having fun in directing a style he usually doesn't delve in. Plenty of action and suspense to hold the audience for the two hour running time, this is a rare movie that doesn't disappoint for one moment.\"\n",
      " b'Hood of the Living Dead and all of the other movies these guys directed look like they got together and filmed this with their buddies who have zero talent one afternoon when they were bored (lines are completely unrehearsed and unconvincing). I find that 95% of amateur movies and 90% of home video footage is better than this film (although the similarities between them warrant the comparison). \"Hey lets see if anyone is dumb enough to buy our movies!\". Hopefully nobody ELSE wasn\\'t. My apologies to those involved in the flic as this review is somewhat harsh but i was the dope who read your fake reviews and purchased the movie.'\n",
      " b'This episode from the first season slightly edges towards controversy with the whole gender bending angle.I imagine there would have been a significant amount of editing before this was finished to appease censors.<br /><br />The story itself is original and well written and the directing is complemented by the fine acting.(Nicholas Lea makes his X-Files debut though not as Agent Krycek but as a victim called Michael) I especially enjoyed the built up to and the ending itself.<br /><br />Its different but enjoyable.Ignore any bad reviews you may hear.Watch it for yourself and make judgement then'\n",
      " b\"This has to be one of the best movies to come out of HK in a long time, i was eagerly waiting to get my hands on this movie just looking at the title. Loads of fantastic actors in this show and i was particularly impressed with Sam Lee's impossibly believable insane behavior and Edison's portrayal of a killer machine, which totally reversed his normal idol image. i would definitely recommend to those looking for a stylish and action packed movie. However, i must warn you, this is also an equally depressing movie, as every character in the movie is in some kind of dead end and trouble of their own, and struggling to breathe. Makes you think about what is life about really.\"\n",
      " b'How do comments like the one that was the headline by high school girls even make it on this site, this was the stupidest movie I have ever seen, it was ridiculous, how can any moron sit there and say that just because a movie makes you jump it is a good movie, that might be the most idiotic thing I have ever heard, I could sneak up behind you and go \"Boo\" and it would make you jump, but that does not mean I am qualified to write or direct a movie, not to mention \"they tied everything together at the end\" is not a good reason for a movie to be well received. What kind of movie would it be if they didn\\'t tie everything together, I guess that would make it half a movie, not a whole movie. So basically this idiot girl is complimenting them for finishing the movie, well I love how the youth of today hold the media and production companies to such a high standard. No wonder the political system of this country can get away with whatever they want, we have idiots like that coming up in our country, what would happen if this girl actually ends up leading something someday, that is a scary thought. Get a life and go watch a real movie sometime, try Shawshank Redemption or On the Waterfront, or something like that, and don\\'t comment that it was \"boring\" after you see it, just use what little brain God gave you to analyze it a little.'\n",
      " b'I just read the plot summary and it is the worst one I have ever read. It does not do justice to this incredible movie. For an example of a good summary, read the listing at \"Turner Classic Movies\". Anyway, this was one of my favorite movies as a young child. My sister and I couldn\\'t wait until every April when we could see it on T.V. It is one of the best horse movies of it\\'s time. It is one of those great classics that the whole family can watch. The romance is clean and endearing. The story line is interesting and the songs are great. They don\\'t make movies like this anymore. Good acting and not over the top. Pat Boone and Shirley Jones are at their best, along with many other great character actors.'\n",
      " b'I have never seen such terrible performances in all my life.<br /><br />Everyone in the entire film was absolute rubbish.<br /><br />Not one decent actor/actress in the whole film, it was a joke.<br /><br />Reminded me of drama at school...'\n",
      " b'I borrowed this movie because not only because its gay theme but the thought of role playing really intrigued me. I was pleasantly surprised that it was shot in San Francisco since I live near SF. And of course it was nice to see shots of the Castro district (although the castro to me really caters more to gay male than female). But other than that I can\\'t really recommend this movie. The characters aren\\'t really developed for me to care and when they finally started to get to the \"role playing\" I was already bored out of my mind. And the role playing scenes that I did see were a bit embarrassing to watch. The acting leaves something to be desired. Needless to say I didn\\'t finish the movie. I\\'d skip this one.'\n",
      " b\"There are good ways to make a movie and bad ways and this very much the former. This short caper exacts nothing more than what it gives to the audience. It presents a simple story, told very plainly with enough wisecracks to keep you going, then just gets better and better. Clooney's cameo is funny and very welcome but the leads including Sam Rockwell and Luiz Guzman can easily make it on their own. Likeable and funny, hilariously so towards the end, Welcome to Collinwood is a welcome addition to the heist genre.\"\n",
      " b'In what must be one of the most blood-freezing movies ever, a transvestite is murdering people in New York, and the answer to everything may not be what people suspect. One can see how Brian DePalma takes some influence from Hitchcock with camera angles and stuff. Michael Caine plays a most thought-provoking character, while Angie Dickinson is basically a bored rich woman with a bad hairdo. Keith Gordon (who later starred in \"Christine\") is probably the most interesting character in the movie. But you can\\'t really understand this movie without seeing it. And after seeing it, you may never know just whom you can trust. Also starring Nancy Allen and Dennis Franz.'\n",
      " b'I purchased this film on the cheap in a sale, having read the back of the DVD case and assuming that either way I can\\'t lose, it if was rubbish then no loss, if it was any good then bargain...<br /><br />Then I watched it...<br /><br />I am normally a fan of Christopher Walken, but in this film he commanded very little screen presence, seeming not to do a whole lot, even the death of his friend near the beginning which sparks off the \"action\" in the plot seems to affect him very little, and his eventual revenge is just boring and undramatic.<br /><br />Normally a film which has themes as grand as revolution and revenge are able to capture the audience and snare them into feeling something for the characters, however watching this film felt more like seeing a series of confused, and almost random events that loosely tied together towards it\\'s eventual conclusion...<br /><br />At this point I wept...<br /><br />I thought this film was the most horribly painful piece of viewing I have ever been subjected to, the scene where the pilot sacrifices himself by refusing to jump out the explosive laden truck due to not wanting to kill any civilians is not so much tragically sad as it is unnervingly horrible and painful, although not quite as bad as the emergency surgery on the wounded girl. The acting was poor all round, the script and story was weak, the \"action\" was even weaker, and the \"visuals\" were but bluntly not all that visual. To summarise there are films which are good, films that are bad, films that are so bad they are good, films that are terrible...<br /><br />And then on a whole new level is \"McBain\"'\n",
      " b\"Being a huge fan of the Japanese singers Gackt and Hyde, I was terribly excited when I found out that they had made a film together and made it my mission in life to see it. I was not disappointed. In fact, this film greatly exceeded my expectations. Knowing that both Gackt and Hyde are singers rather than actors, I was prepared for brave yet not really that fulfilling performances, but am delighted to say that both of them managed to keep me captivated and believing the story as it went on. Moon Child has just the right amount of humour, action, romance and serious, heart-wrenching moments. I can't say that I've ever cried more at a film and these more tender moments are admirably acted by the pair, in my opinion, definitely proving their skills as actors. The fight scenes are absolutely stunning and although there are a few moments of uncertainty to begin with, you are quick to get into the movie and begin to bond with the characters. I thoroughly recommend this film to anyone, especially those who are fans of Gackt and Hyde.\"\n",
      " b\"For Romance's sake, as a married man. The following two films are recommended.<br /><br />1. Brief Encounter by David Lean (1945), UK<br /><br />Well, when a woman goes to a railway station, something may happen. And it happened! How she longed to be there, in a little tavern waiting for the man of her dreams. But she was married... the man was a stranger to the fantasizing woman<br /><br />2. Xiao Cheng Zhi Chun by Fei Mu (1948), China<br /><br />Well, when a woman goes to the market to buy fish, grocery and medicine, passing through the ruins of an ancient wall in a small town, there is much to think about, about the melancholy of her life, her sick husband in self-pity and lack of future...Just when a jubilant young doctor arrived, something happened... the doctor was a high school honey of the fantasizing woman<br /><br />In both movies, from great directors of UK and China, the passion vs restraint was so intense, yet in the end the intimate feelings had not developed into any physical contacts. That leaves you with a great after-taste, sniffing it intensely without biting it.\"\n",
      " b'I have screened this movie several times here at college, and every time I show it, the number of people watching with me grows exponentially... in addition to the virgins, anyone I\\'ve already shown it to NEEDS to see it again! It takes a little while to get into it, but by the end the whole room is screaming, shouting, yelling, rewinding scenes repeatedly, repeating dialogue, and just totally and completely engrossed in the moviegoing experience that is Pia Zadora in \"The Lonely Lady\"! Scene after scene after scene of the most ineptly filmed, poorly written, horribly acted TRASH is thrown at you in an all-out assault that ranks as the campiest thing I own (no small statement, friends). For me nothing compares 2 U, Pia... and I don\\'t suppose I\\'m the only one who\\'s ever felt this way!'\n",
      " b'Somebody owes Ang Lee an apology. Actually, a lot of people do. And I\\'ll start. I was never interested in the Ang Lee film Hulk, because of the near unanimous bad reviews. Even the premium cable channels seemed to rarely show it. I finally decided to watch it yesterday on USA network and, wow....<br /><br />SPOILERS FOR ANG LEE\\'S HULK AND THE INCREDIBLE HULK <br /><br />Was it boring! I almost didn\\'t make it through Ang Lee\\'s Hulk. Eric Bana was expressionless, Nick Nolte was horrible, Sam Elliott was unlikeable (and that\\'s no fun, he\\'s usually a cool character). In fact, I honestly think they chose Eric Bana because his non-descript face was the easiest to mimic with computer graphics - and it was clear that the Ang Lee Hulk was meant to facially resemble Bruce Banner in his non-angry state. When Hulk fought a mutant poodle I was ready to concede Hulk as the worst superhero movie ever.<br /><br />But then something happened. About 3/4 of the way through this tedious movie, there was a genuinely exciting and - dare I say it - reasonably convincing - extended action scene that starts with Hulk breaking out of a containment chamber in a military base, fighting M1 tanks and Comanche helicopters in the desert, then riding an F22 Raptor into the stratosphere, only to be captured on the streets of San Francisco. This was one of the best action sequences ever made for a superhero movie. And I have to say, the CGI was quite good. That\\'s not to say that the Hulk was totally convincing. But it didn\\'t require much more suspension of disbelief than is required in a lot of non-superhero action movies. And that\\'s quite a feat.<br /><br />Of course, the ending got really stupid with Bruce Banner\\'s father turning into some sort of shape-shifting villain but the earlier long action sequence put any of Iron Man\\'s brief heroics to shame. And overall, apart from the animated mutant dogs, it really did seem like the CGI in Hulk tried hard to convince you that he was real and really interacting with his environment. It was certainly better than I expected.<br /><br />OK, but what about The Incredible Hulk? Guess what... It\\'s boring too! It has just a few appearances by the Hulk and here\\'s the thing - the CGI in this movie is horrible. Maybe the Hulk in Ang Lee\\'s version looked fake at times and cartoonish at others - but it had its convincing moments also. The Incredible Hulk looked positively ridiculous. It had skin tone and muscle tone that didn\\'t even look like a living creature, just some sort of computer-generated texture. It was really preposterous. The lighting, environment and facial effects didn\\'t look 5 years newer than Ang Lee\\'s, they looked 10 years older. And there really is no excuse for that. We truly are living in an era where computer programmers can ruin a movie just as thoroughly as any director, actor or cinematographer ever could.<br /><br />Worse, the writer and director of this movie seemed to learn almost nothing from Ang Lee\\'s \"failure\". All the same mistakes are made. Bruce Banner is practically emotionless. The general is so relentlessly, implausibly one-dimensional that he seems faker than the Hulk. The love interest is unconvincing (I have to give Liv Tyler credit for being more emotional than Jennifer Connelly, though both are quite easy on the eyes). Tim Blake Nelson overacts almost as much as Nick Nolte, even though he\\'s only in the movie for a few minutes. The Hulk really doesn\\'t do much in this movie, certainly not any more than in Ang Lee\\'s version. The Incredible Hulk was slightly more fast-paced, but since nothing really happened anyway that\\'s not worth much. Oh yeah, the villain is every bit as phony looking as the Hulk. He\\'s actually much more interesting as a human than as a monster. <br /><br />This is how I can definitively say Ang Lee\\'s version was better: if I ever have the chance to see Ang Lee\\'s version again, I might be able to sit through it to see the good action sequences, or else to try to appreciate the dialogue a little more (more likely I\\'d just fast forward to the good parts). But there is absolutely not a single scene in The Incredible Hulk that is worth seeing once, let alone twice. It is truly at the bottom of the heap of superhero movies. The cartoonish CGI is an insult to the audience - at least in Ang Lee\\'s version it seems like they were trying to make it realistic (except for the giant poodle, of course).<br /><br />It is absolutely mind-boggling how the filmmakers intended to erase the bad feelings associated with Ang Lee\\'s Hulk by making almost exactly the same movie. <br /><br />It is to Edward Norton\\'s credit that he seems to be distancing himself from this film.'\n",
      " b'Ulysses as a film should in no way be compared with the novel, for they are two entirely different entities. However, that being said, the film still manages to maintain many of the elements that made the book work, but since it is a visual medium, it is more difficult to pull of stream-of-consciousness. I think this is the best film they could have made with the material... and this is from someone that routinely rants about films not being like their literary counterparts. I recommend the book, but the movie is still entertaining.'\n",
      " b'Just saw this at the Chicago Film Festival - avoid it at all costs unless you have sleep problems. It is a film filled with pretensions - it opens with a minor quote from \"Hiroshima mon amour\" and it\\'s all downhill from there. Camera work - imagine a child trying to imitate Wong Kar Wai. Story line - Smokey Robinson and the Miracles\\' \"The Love I saw in You Was Just a Mirage\" expanded from 3 minutes to over 2 hours but filled with repetition. For butt numbing pain this film ranks with the benches at the Methodist church my parent dragged me to when I was a kid. I want 2+ hours of my life refunded. Julian Hernandez\\'s promoter prefaced the viewing with comment that the film was \"controversial\" - that is true only for the film\\'s narcotic effect.'\n",
      " b'I bought a set of 4 DVDs for 10 bucks at my local Suncoast, which contained this movie and three other trashy horror flicks (including its sequel \"Witchcraft XI\"). So basically I paid the rock bottom price of $2.50 for this movie, if you do the math. I can\\'t exactly say I was ripped off. I have a thing for trashy horror movies, but this is the kind of trash that gives trash a bad name. The budget couldn\\'t be over $1,000 (though it appears as if they spent a total of $1.50). I know it\\'s a low-budget film, but that\\'s no excuse for totally uninspired camerawork. The film \"Blood Cult,\" though not very good, was made for an extremely low budget and still had fairly good camerawork and acting. The acting in this movie is the definition of \"effortless,\" especially from that muscular guy with the Texas accent. Everyone is pretty much reading their lines off the page. You can take that figuratively or literally. I wouldn\\'t be surprised if the script was off-camera as they were performing. I said before that I\\'ve never seen a bad English actor. This movie has quite a few bad ones. And though English movies aren\\'t always good, they always seem to have at least a level of sophistication, which is why I don\\'t see why any Englishman, or Englishwoman, would volunteer to do a home-video-style schlock flick like this. Did Merchant Ivory put a hold on their casting calls? Usually, I think people are too hard on directors and actors. Even some of the worst movies in Hollywood have some level of professionalism in the directing, acting and cinematography departments. Even when you watch a movie like \"Glitter\" you can\\'t honestly say it looks like a third-grader shot those scenes (though a third-grader could\\'ve written the script). I\\'ve seen home movies that are shot better than \"Witchcraft X,\" and that\\'s no exaggeration whatsoever. Even the gore is minimal since the filmmakers only had money to buy some fake blood on sale at Party City. Not a single effort was put into making this movie--let\\'s just sum it up like that. You get the picture. There\\'s a good deal of nudity, though that doesn\\'t save it. However, I must say that girl with the red-orange hair, who\\'s either naked or wearing a cleavage-popping outfit throughout the film, is really hot! <br /><br />My score: 1 (out of 10)'\n",
      " b\"This was quite possibly the worst film I've ever seen. The plot didn't make a whole lot of sense and the acting was awful. I'm a big fan of Amber Benson, I think she's usually a wonderful actress, I can't imagine why she decided to do this film. Her character, Piper, is drunk for almost the whole film, with the exception of the opening scene. On the plus side, there was several points in the film where the acting was so bad, I actually laughed out loud. But despite that, I would not recommend this film to anyone. It's only 80 minutes long, but that's 80 minutes of your life that you will have completely wasted.\"\n",
      " b'This is a great Canadian comedy series. The movie tells of how the stars Jean Paul Tremblay-(Julian) and the head writer of the show and his buddies Rickie and Bubbles play it over the top in what is a true life satirical look at trailer parks and the denizens of said trashy hoods. The movie will tell you why Rickie and Julian begin doing their more advanced forays into the world of crime. WHY and the reasons behind everything would be a spoiler so I shall not give the real reasons behind their more brilliant escapades. Their friend and oft-time partner(Bubbles) is a brilliant character. The whole show is brilliant and missing the movie will not affect the way you see the sit-com one bit. It is a comedy with a capital C and a brilliant satire on trailer park living and small time crooks with small time ideas but big time dreams. If you ever have the opportunity to watch--buy--steal this program grab it. You will be glad you did. And to my American friends---It will break you up also. 10 out of 9. Brilliant. TV how TV should be.'\n",
      " b'Mulva is put in a sugercoma by Teen Ape. When she awakens she\\'s considerably hotter (the parts played by Debbie Rochon in this sequel), and out for revenge on those that did her wrong. As the sub-title and box art implies this is indeed a take off on the \"Kill Bill\" films, but this being a Chris Seaver\\'s film, it\\'s a wildly incompetent satire (and I use that last term extremely loosely) I\\'d love to say this is better than the first film, but truth be told I was so impossibly drunk off my ass when I saw the previous film that I can\\'t even hope to compare the two at this point in time. But I must have enjoyed it since I bought the sequel (see when I\\'m drunk I like, for lack of a better word, complete and total crappy movies) There are a few laugh out loud moments (very few) but I do remember Bonejack being funnier though. Well at least at slightly over an hour, it IS over mighty quickly for what it\\'s worth.<br /><br />My Grade: D+ <br /><br />DVD Extras: Audio commentary by Director Chris Seaver, actress Debbie Rochon, and the Lbp gang; Second commentary by Seaver; 31 minute Making of featurette; Lloyd Kaufman\\'s 6 minute tribute to low budget pictures; Fake 2 minute syrup commercial; stills gallery; Promo trailer; and trailers for :Mulva\", \\'Quest for the Egg Salad\", \"Fiilthy McNasty\"1, 2, & 3; \"the Feral Man\", \"the Bonesetter\", \"Midnight Skater\", \"Demon Summer\" & \"Splatter Rampage Wrestling\"'\n",
      " b\"This is a movie which attempts a retelling of Thai history, set in the ancient city of Ayutthaya. I decided to watch this film because I thought it was along the lines of many Thai films I've watched and enjoyed, one that has Thai actors speaking Thai and martial arts craziness. Well, it's none of that. This film is shot entirely in English, is chock full of Anglo actors, and has production values so terrible it is laughably bad....but not funny! Who can we blame for this rubbish? The acting, dialog, and most of the sets were quite bad. Some of the fight scenes looked like they were choreographed by the local high school drama club. The special effects were also mostly bad, but a few were just cheap animation patched onto the screen that provided an especially cheesy effect. It has one large, epic-style outdoor battle scene, where a few thousand extras get to run across a field in costume, but when we see the two armies collide in combat--HA! What a joke! The film does feature a couple of beauties. What a pity they didn't show a little more skin. At least that would have been something for the guys to appreciate. Don't bother.\"\n",
      " b'What do I say about such an absolutely beautiful film? I saw this at the Atlanta, Georgia Dragoncon considering that this is my main town. I am very much a sci-fi aficionado and enjoy action type films. I happened to be up all night and was about ready to call it a day when I noticed this film playing in the morning. This is not a sci-fi nor action film of any sort. Let me just start out by saying that I am not a fan of Witchblade nor of Eric Etebari, having watched a few episodes(his performance in that seemed stale and robotic). But he managed to really win me over in this performance. I mean really win me over. Having seen Cellular, I did not think there was much in the way of acting for this guy. But his performance as Kasadya was simply amazing. He was exceedingly convincing as the evil demon. But there was so much in depth detail to this character it absolutely amazed me. I later looked it up online and found that Eric won the Best Actor award which is well deserved considering its the best of his career and gained my respect. Now I keep reading about the fx of this and production of this project and let me just say that I did not pay attention to them (sorry Brian). They were very nicely done but I was even more impressed with the story - which I think was even more his goal(Seeing films like Godzilla with huge effects just really turned me off). I could not sleep after this film thinking it over and over again in my head. The situation of an abusive family is never an easy one. I showed the trailer to my friend online and she almost cried because it affected her so having lived with abuse. This is one film that I think about constantly and would highly recommend.'\n",
      " b\"This review is based on the dubbed Shock-o-Rama video released on an undeserving world in 2002. How bad is it? It's awful, which is what a '1' represents on the IMDb scale--but it's much worse than that. It's nice to imagine that an original German-language print might improve matters--the comedic English-language dubbing isn't funny at all--but truthfully, this is one of the worst amateur films of any genre you're likely to see. The zombies in the film are as slow and clumsy as ever, and they don't seem to have the ability to speak or think about anything beyond their next meal. However, they're also intelligent enough to operate chainsaws and malicious enough to know that western taboos about genitalia will no doubt enliven their dinner table conversation. George Romero's Land of the Dead posited a zombie nation that retained a shred of social coherence; here, zombies are nothing more than an empty canvas for the perverse imaginings of director Andreas Schnaas. Utterly without redeeming social value, and even worse, entirely lacking as entertainment, Zombie '90 is a bad joke on anyone who wastes money on it.\"\n",
      " b\"I watched this film in shire joy.<br /><br />This is possibly one of the best films of all time. It has a timeless value, you can get so much out of it it's amazing. There are parts that are moving, funny, and just great.<br /><br />All aspect are spot on, the portrayal of the story is perfect, every detail is 100% genuine, even small Irish subtleties have been covered.<br /><br />The use of low and high shots gives two great views on Cristy (look out for that).<br /><br />Daniel Day-Lewis's performance is incredible. I've never seen an actor do that, ever. It really is amazing.<br /><br />And it's so great to watch, it flows so well, it's probably the closest thing yo can get to real life experience. I love it.<br /><br />If you haven't seen it, you should see it. Don't have any doubts on it, there is something there for all.\"\n",
      " b\"Good old black and white Graham Greene based people in dangerous times doing heroic and mysterious things. Hardly a shot fired or a punch thrown and a hundred time more interesting than the glop that's being minted by Hollywood today. Bacall lights up the screen of course and Boyer is entirely engaging. They don't make movies like this any more.\"\n",
      " b\"Extremely poor action film starring the ever wooden Dolph Lundgren and Brandon Lee trapped in a sidekick role that never seems to gel. The action is at best average, a bit of nudity chucked in and yes Tia Carrera does use a body double! <br /><br />The set-up is the usual renegade cop forced to break in a new partner on a big case, the makers at least try to give the formula a twist making Lundgren the cop with Oriental values and Lee the modern city slicker but there is zero character development making it almost comical, Lundgrens oriental warrior outfit for the big showdown has to be seen to be believed. The action sequences are by the numbers and Lee(who would go on to make the excellent The Crow) is never given the scope to show off any particular martial arts brilliance. But given his illustrious parentage he must have been under a hell of a lot of pressure and was far better served not having to live up to his father by taking on a very different role in The Crow which showed what a unique actor he may have become if not for his tragic and early death.<br /><br />Unless your a hardcore Lundgren fan or a fan of poor 80's style action movies (think Cobra etc.) then avoid.<br /><br />Poor 3/10\"\n",
      " b'I was interested in the topic, and only fans of Drew Barrymore\\'s dancing on David Letterman\\'s desk will find anything remotely interesting in it. OK, she shows some breast (or maybe a body double does). The plot is slashed to bits and the acting is horrible. Neither lead has any material to work with, as the direction of the film leads nowhere. Don\\'t waste your time. See Donnie Darko instead if you want a creepy Drew Barrymore film, and if you want to see another, skip this and see Darko again.<br /><br />The treatment of the Doppelganger legend is absolutely criminal as well. Refer to Charles Williams\\' novel \"Descent Into Hell\" for something worth considering instead. This is just an excuse to make a B film to go straight to video and suck some life out of people at Blockbuster.<br /><br />What makes any of these people think the acting here was praiseworthy? Give me a break.'\n",
      " b\"Critics need to review what they class as a quality movie. I think the critics have seen too many actions films and have succumbed to the Matrix style of films. Europa is a breath of fresh air, a film with so many layers that one viewing is not enough to understand or appreciate this outstanding film. Lars von Trier shows that old styles of filming can produce marvellous cinema and build drama and tension. The back projection effect he uses during the film arouses and enhances the characters, and the focus of the conversation they are having. Other effects he uses such as the colour and black and white in one scene much like Hitchcock and the girl with the red coat grabs attention and enhances the drama and meaning of the scene. The commentary is superb and has a hypnotic effect, again maintaining the focus on the central characters in the scene and there actions.<br /><br />I could talk about the effects more but I think you all would agree they push this film into a category of its own, and really heighten the drama of the film. A film to buy if you don't own already and one to see if you have not.<br /><br />10/10 Don't miss this artistic noir film from one of the great film directors.\"\n",
      " b'An interesting thriller that has Paul Winfield as a detective on the case of a murder. Paul Winfield was an underrated actor who pulled off all his roles with such ease, it was hard to tell the man was even acting. Maybe most known by younger viewers as the voice/narrator of \"City Confidential\", Winfield ends his career with a so-so movie; but as always, Winfield shines. A treat to watch.<br /><br />Erika Eliniak is well, Erika Eliniak, nice to look at but leaves a lot to be desired in the acting department. Though, to be fair, this is one of her better efforts.<br /><br />Bottom line: a watchable thriller that shouldn\\'t be missed by any Paul Winfield fan. A decent telefilm to help send Paul Winfield off to celluloid heaven. What an actor. He will be missed.'\n",
      " b'Hundstage is an intentionally ugly and unnerving study of life in a particularly dreary suburb of Vienna. It comes from former documentary director Ulrich Seidl who adopts a very documentary-like approach to the material. However, the film veers away from normal types and presents us with characters that are best described as \"extremes\" \\xc2\\x96 some are extremely lonely; some extremely violent; some extremely weird; some extremely devious; some extremely frustrated and misunderstood; and so on. The film combines several near plot less episodes which intertwine from time to time, each following the characters over a couple of days during a sweltering Viennese summer. Very few viewers will come away from the film feeling entertained \\xc2\\x96 the intention is to point up the many things that are wrong with people, the many ills that plague our society in general. It is a thought-provoking film and its conclusions are pretty damning on the whole.<br /><br />A fussy old widower fantasises about his elderly cleaning lady and wants her to perform a striptease for him while wearing his deceased wife\\'s clothes. A nightclub dancer contends with the perpetually jealous and violent behaviour of her boy-racer boyfriend. A couple grieving over their dead daughter can no longer communicate with each other and seek solace by having sex with other people. An abusive man mistreats his woman but she forgives him time and again. A security salesman desperately tries to find the culprit behind some vandalism on a work site but ends up picking on an innocent scapegoat. And a mentally ill woman keeps hitching rides with strangers and insulting them until they throw her out of the car! The lives of these disparate characters converge over several days during an intense summer heat wave.<br /><br />The despair in the film is palpable. Many scenes are characterised by long, awkward silences that are twice as effective as a whole passage of dialogue might be. Then there are other scenes during which the dialogue and on-screen events leave you reeling. In particular, a scene during which the security salesman leaves the female hitch-hiker to the mercy of a vengeful guy - to be beaten, raped and humiliated (thankfully all off-screen) for some vandalism she didn\\'t even do - arouses a sour, almost angry taste. In another scene a man has a lit candle wedged in his rear-end and is forced to sing the national anthem at gunpoint, all as part of his punishment for being nasty to his wife. While we might want to cheer that this thug is receiving his come-uppance, we are simultaneously left appalled and unnerved by the nature of his punishment. Indeed, such stark contrasts could act as a summary of the whole film - every moment of light-heartedness is counter-balanced with a moment of coldness. Every shred of hope is countered with a sense of despair. For every character you could like or feel sympathy for, there is another that encourages nothing but anger and hate. We might want to turn away from Hundstage, to dismiss it as an exercise in misery, but it also points up some uncomfortable truths and for that it should be applauded.'\n",
      " b\"I was about 11 years old i found out i live 0.48 miles from the uncle house. the uncle house is on Westway Dr, deer park, TX. they have added homes since the movie was made. i don't know the house number but you can go look it up. i am now 21 and i enjoy watching this movie. the bar on Spencer is no longer their. Pasadena ISD wants to build a school their. I drove by the house last week the house still looks great. My dad and uncle would go to the street where the house is and watch the actors come in and out of the house trying to make the movie. where john cross over the railroad cracks they have made 225 higher. when i hear about john loesing his son i start thinking about when he made urban cowboy he was 26 or 25 at the time.\"\n",
      " b'Adrianne, should really get a life-without Mr. \"Brady\". She nauseates me, and has been one of the main reasons why I know longer tune in to the show. It\\'s pretty brainless show, and every little argument or disagreement seems to be put under the scope and analyzed to death. This makes them look/sound they are anything but ready for marriage, and yet, I know these disagreements are all part of life. I guess to some people this is entertainment. If this happens to fall into next season I will feel sorry for anyone who has nothing better to do with their life but watch this trash. Though I would not be terribly surprised. can\\'t even stand the commercials for this show anymore! I hope they\\'re getting enough money to constantly embarrass themselves in front of a camera week after week. However, the \"A\" girl has one heck of great butt!'\n",
      " b'Worst movie ever made!!! Please see the Real movie reviews from the pros on this movie.Check Rotten Tomatoes on the web for some good independent reviews on this film. The comments made on this site are apparently from folks with some financial interest in this film. I find the positive comments very misleading. I find it amazing how the negative comments are so bad against this movie and the positive comments sound like an Academy Awards Speech. Don\\'t waste your hard earned money!!!!!! This Film is retarded!! I can\\'t believe a film like this would ever be made. Why would Hollywood waste their time on such junk? This film is an attempt at nothing. I ask myself what looser would actually sink their money producing such trash. I went to blockbuster and the attendant even told us not to waste our time or money. I didn\\'t listen and I did waste my time and cash. Please don\\'t make the same mistake! It really is the \"Worst movie ever made!\"'\n",
      " b\"The story and music (George Gershwin!) are wonderful, as are Levant, Guetary, Foch, and, of course, Kelly. One thing's missing, and that thing is a good leading lady. I'm sorry, Leslie Caron bothers me. Anyway, despite her, the plot moves along nicely with the famous (and deservedly so) Ballet. Oh the colours, the dazzling reds, blues, greens, and yellows. Musn't forget the beiges as well. ; ) I just adore the contrast between the Beaux Arts Ball (completely black and white costumes) and the ever-so-brilliant Ballet.<br /><br />So I suppose what I'm trying to say is this: Please, by all means see it, and enjoy it, because though it isn't the best, it is MARVELOUS. But be sure not to forget that other Gene Kelly musical with the 20 year old girl that was catapulted to stardom just afterward.\"\n",
      " b\"Vampires Vs. Zombies starts with the breaking news that the unidentified disease that is spreading across America leaves the sufferer with homicidal & cannibalistic tendencies... Travis Fontaine (C.S. Munro) & his teenage daughter Jenna (Bonny Giroux) listen to the radio as they drive along the isolated backwoods roads to try & escape the disease when Travis runs over a guy who I assume is meant to be a zombie. Slightly further down the road he stops to help Julia (Brinke Stevens) & her teenage daughter Carmilla (Maratama Carlson) who are waving at the side of the road, at this point there is also a third teenage girl named Tessa (Melanie Crystal) sitting in the back of Julia's car bound & gagged. To me this situation would seem strange but Travis, like the trooper he is, takes it all in his stride & agrees to 'take' Carmilla off Julia's hands &, well I don't know actually. So, with a complete stranger, Travis drives off leaving Julia & Tessa. Carmilla seems like a nice girl but she turns out to be a Vampire & she likes to bite people & turn them into Vampires, oh & she's partial to a bit of lesbianism too. Travis, Carmilla & Jenna continue to travel while some guy who calls himself The General (Peter Ruginis) who appears to be some sort of Vampire killer & probably has something to do with it all but the film is such a mess it doesn't really matter & I really don't know how to carry on this plot outline as my head hurts just thinking about it...<br /><br />Co-edited, co-executive produced, written & directed by the supremely untalented Vince D'Amato Vampires Vs. Zombies is one of the worst horror films ever & therefore one of the worst films ever period. The script by D'Amato was apparently based on a classic story entitled 'Carmilla' by Sheridan Le Fanu (he should sue) & is an absolute mess, the holes in the plot & logic are so big you could drive a tank through them! What is the disease that turns people into zombies? Why is Carmilla a Vampire? Who is Julia to her? Who the hell is The General? What does he want? Where are Travis & Jenna going? How can Travis run a man over & yet not have the slightest bit of human emotion over it? What's with the mental ward at the end? There are also some confusing & unnecessary dream sequences just to annoy the viewer even more. There are just so many things wrong with this film, the narrative doesn't make a blind bit of sense, the concept is terrible & never really explained properly plus it's incredibly boring. I have not one positive thing to say about Vampire Vs. Zombies, not one. Forget about any Vampires fighting Zombies because it just doesn't happen, tell me again why is this film called Vampires Vs. Zombies?<br /><br />Director D'Amato has served up one of the most incompetent, rubbishy, badly made, poorly thought out & excruciatingly painful viewing experiences ever made. Vampires Vs. Zombies really has no redeeming qualities at all, there is not one single aspect that I can praise. The gore is really fake looking, there are some blood splats which look like red water, some really cheap staking effects & a half decent climax where the zombies feast on Carmilla's & Jenna's intestines, this fairly gory scene is probably the best part of the whole wretched film but it only lasts for a couple of minutes & in no way makes up for the other turgid 85.<br /><br />The budget on Vampires Vs. Zombies must have been small, in fact did it even have a budget because most of it is set on a road in a couple of cars. This is one of the most badly made horror films it's been my misfortune to watch, the entire thing just sucks. The acting is predictably awful, & I mean awful.<br /><br />There isn't much else left to say, Vampires Vs. Zombies is easily one of the worst films ever made. The (V) next to the title on the IMDb's main page for Vampires Vs. Zoimbies indicates that it went straight to video, well that's far too good for this pile of crap as it deserves to go straight on the nearest fire.\"\n",
      " b'This movie had no parts that were hilarious, mostly just average funny units, but it did not have any parts that were really bad either. The worst part was the voice of Sid. His slothy slur was just too much for me. By about 5 minutes in I was sick of hearing him talk. Aside from the annoying sloth voice the movie was good. There were numerous side jokes which if you catch them make the movie much better. This is a good movie for kids. It has enough in it to keep adults content and enough in it to entertain kids. This one is definitely worth renting if you have kids and want to watch a movie with them.'\n",
      " b'Joel schumacher Made a heck of a choice when he decided on this cast and this script. The story is well written and well laid out, and this entirely new cast of 10 or 12 central characters was absolutely brilliant. It seemed that there were 6 \"leads\" and about a half dozen supporting, and by far this is the best thing about the movie, the fresh young faces of tomorrow. It has been a long time since hollywood has touched the controversial vietnam war films,which says something for the\"story that needed to be told\"(as stated by schumacher) and Tigerland lands in that handful of top war movies period. Yet it can not be labeled as a war movie because it seemed to be based more on the human spirit of Bozz and the others. I Think anyone who just wants to see a good film with out all of the special FX, but just good, gritty drama should go see Tigerland, obviously Shumachers Best works in the past 8 years.'\n",
      " b'This was not the worst movie I\\'ve ever seen, but that\\'s about as much as can be said about it. It starts off with some good atmosphere; the hospital is suitably sterile and alienating, the mood is set to \"eerie\". And then...nothing. Well, somethings. Just somethings that clearly don\\'t fit in...and no effort is made to clarify the connection between the bizarre and yet not particularly intimidating critters, and the hospital they\\'ve taken over. I mean, come on, biker duds? Some band watched a bit too much Gwar.<br /><br />My personal favorite was the head demon, who looks rather a lot like a middle-aged trucker desperately attempting menace, while simultaneously looking like he\\'d really like prefer to sag down on an afghan-covered couch, undo his belt, pop a can of cheap beer (probably Schlitz), and watch the game. Honestly, I\\'ve seen far scarier truckers. At truckstops. Drinking coffee. WWWwoooooohHHHHHoooooooo!!!! Scary!!<br /><br />The other monsters are even more cartoonish, and even less scary. At least, on the DVD, the videos give some explanation of their presence in the hospital...they apparently just randomly pop up in places, play some bippy \"metal\", and cause people to be dead a bit. Barring a few good special effects, and acting that is not entirely terrible given a lack of decent writing, there\\'s just nothing here. It\\'s a background-noise movie only.'\n",
      " b\"I was so surprised by how great The Man In The Moon truly was.I mean at first I was kinda expecting a cheesy, and predictable film, but I decided to put that aside when watching.Well, when it was over I was just left stunned(mainly in tears), by how great The Man In The Moon turned out to be.This movie is so entertaining and is so aware of its tone, and its just a fabulous film.The acting was great especially from Reece Witherspoon(who was so cute and lovable), and everyone else.There wasn't anything that really bothered me, I felt the ending kinda predictable, but very well done at that.Also I felt some things to be plain or as if it had been done before, but still a great film.Overall I must say I don't to much to say about this film, not that it was bad, its just a film you either like or don't like.I would however recommend this to any and everyone, even if you don't like these type of films, its still an enjoyable film.<br /><br />8.7 out of 10 stars\"\n",
      " b'Unusual? Yes!<br /><br />Unusual setting for an American wartime movie, New Zealand.<br /><br />Unusual subject matter, four sisters and their relationships with American soldiers, from one bearing the illegitimate child of the dead son of a Senator, to another living with seven Marines (one at a time) before being murdered by her returning POW New Zealander husband.<br /><br />Unusual to see Paul Newman deliver such a poor performance so soon after his unforgettable role as Rocky Graziano in the brilliant \"Somebody Up There Loves Me\".<br /><br />Unusual for two fine \"Stars\" Joan Fontaine and Jean Simmons, to leave so little of themselves on a movie.<br /><br />Unusual that I could be bothered to write a review of such a poor film, give it a miss!'\n",
      " b'Savage Island (2003) is a lame movie. It\\'s more like a home video shot with very minimal lighting and horrid acting. Not only that the storyline and script was wretched. I don\\'t know why this movie was made. I have seen a lot of flicks in my time and the ones I really hate are movies that make me angry. This one made my blood boil. The situations were inane at best. If I made a movie like this it would have been a short. Really because those backwood \"idjits\" wouldn\\'t have been in the picture.<br /><br />Don\\'t be fooled by the cover on the D.V.D. I am an avid watcher of bad cinema. But this movie is virtually unwatchable. I don\\'t mind movies being shot on D.V. but if you\\'re going to do that make the movie enjoyable, not some tired retread of superior horror films (sans Wrong Turn).<br /><br />I have to not recommend this waste of disk. If you come across this one in the rental store pass on by.<br /><br />Movies that make yours truly angry get an automatic 1.'\n",
      " b\"This movie was bizarre, completely inexplicable, and hysterical to watch with friends while drinking in a big empty house. I really love the opening stuff with Lisa wandering about lost in a gorgeous city. I want to be a beautiful stranger lost in some exotic European locale, though maybe not in a low budget horror flick. Definitely get the ending where there are the strangely non-sexual sex scenes that were cut out (in my DVD copy anyway). Don't attempt to understand it, just go along and watch out for the weird bits...which is everything. Don't watch this if you actually want plot or characterization or anything at all to make sense. Pretty beautiful, though you may just give up on this and decide to watch an actual horror movie, like say, Dead Alive.\"\n",
      " b\"Val Kilmer... Love or loath him, sometimes he gets under the skin of a character and pulls out a performance that makes you go 'Hey! This guy is a GREAT actor!' He did in the leather pants of Jim in The Doors and he's done it again in the leather underpants of John.<br /><br />Revolving around the fall and fall of uber porn king John Holmes, Kilmer strutts to his knees as we unravel one of the biggest murder mysteries hollywood has never solved for over twenty years, with Holmes the key suspect to a brutal Manson-style slaughter.<br /><br />What Kilmer does so effortlessly is exhude the low-life of the celebrity, the do anything to anyone craving that overwhelms anyone who had it and then lost it. Go see him, you'll know what I mean.\"\n",
      " b'Basically, a dentist husband-wife team and their 3 daughters deal with infidelity. The premise is interesting, the acting is good, and the music, although sometimes abrupt and without direction, is pretty cool.<br /><br />The problem is the plot. The husband dentist drops his wife off backstage at an opera before the show (she has a minor role) and then walks back in to give her something, but sees her with another man. The rest of the movie deals with his angst about this episode, his visual hallucinations and a macho alter-ego (Denis Leary, a former patient of his) and his fear in confronting his wife lest he will have to \"do something about it.\" I won\\'t tell you the ending, but let me say that the film goes nowhere and the ending is like a sputtering whimper. The motivations of the characters are missing: Why is she cheating on him? He\\'s a dentist, decent looking, good father. The film doesn\\'t say. Who\\'s she doing it with? Don\\'t expect any answers on that either. Why does he want to keep the marriage going in spite of all this? Who knows. What purpose does all the kids vomiting serve? Where is this film going? Good performances by Campbell Scott and Hope Davis (and Denis Leary as comic relief)are completely wasted by this stilted nonsense which doesn\\'t know if it wants to be American Beauty or a family film. A root canal is more interesting. Avoid it.'\n",
      " b\"Massive multiple chills down the spine! I'm surprised there's people who didn't like it! I saw it at 10 o'clock in the morning and still got scared stiff! And I've seen hundreds of thrillers/horror movies! For crying out loud,I'm 22!!! I mean, OK, voice acting, not particularly good, probably even b-movie-ish. But the genuine look of terror, the sound effects, the flow! From the very start, hitting you again and again with relentless, unforgiving, terrorising scenes! So many clich\\xc3\\xa9s yet none fails to surprise/scare! You know it's coming, it's coming, it's coming, BOO! and you still jump off the chair. Grab a pillow and a blanket, call your closest friend over and do not watch it at night! Hats off to the Japanese!\"\n",
      " b\"This has got to go down as almost one of the worst movies of all time. Awful acting, awful script... and they were the good points! One to Definitely miss! The jokes, if you could call them that, were so predictable as to be pathetic. Pamela Anderson is still relying on her body to detract from the fact that her acting is just as plastic! I sat willing to give it a chance, hoping that it was going to improve which, alas, it didn't! If it was a choice between this and a book, I suggest you settle down for a good read! I like Denise Richards, which is why I gave this movie a go, but why she has let her self be cast in this movie is beyond me!\"\n",
      " b'I love the book, \"Jane Eyre\" and have seen many versions of it. All have their strong points and their faults. However, this was one of the worst I have seen. I didn\\'t care about Jane or Mr. Rochester. Charlotte Gainsbourg (Jane) was almost tolerable and certainly looked the plain part, but she had no emotion in any of her lines. I couldn\\'t imagine what Mr. Rochester saw in her. <br /><br />That brings us to Mr. Rochester. William Hurt had even less emotion than Jane, if that were possible. How two such insipid people could fall in love is a mystery, but it certainly didn\\'t hold my attention. Perhaps the director (Zeffrelli) fell asleep during the production.<br /><br />The Timothy Dalton (too handsome for Mr. Rochester!) version is far more faithful to the book, but Ciaran Hinds plays the perfect Mr. Rochester in the 1997 A/E version (which is NOT all that true to the book).<br /><br />Trying to find something positive about this movie: Geraldine Chaplain was perfect in her role.'\n",
      " b'This is the worst movie I have seen for years! It starts ridicoulus and continues in the same way. I thnik when is something going to happen in this film,,,, and the the acting is worse. The ending lifts it a bit and saves the movie from a total flop. Mark Wahlberg is a bad actor in a bad movie. Sorry Tim Burton Batman was good but this one sucks.'\n",
      " b\"Unless you're twelve, this movie really isn't worth it. It's obviously a low-budget film with B actors, and with a genre like fantasy that sometimes requires intense CGI work that's not good. I knew it would be bad when I rented it. I enjoy laughing at bad movies. I didn't know how bad though. It's bearable, until after hour 2, then it really starts to burn. Fighting styles go between normal fighting that obey the laws of physics, and wire-fighting. There's no real explanation for the transitions. It has a plot, but once again, it's obviously a kid's movie. It seems like there are explicit moral lessons of the day that are being conveyed, like Sesame Street or something. It's bearable. But much better if you're, say, nine.\"]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-eed3d27c636c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Review: {text_batch.numpy()[i]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Label : {label} ({class_names[label]})'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in range(3):\n",
    "        print(f'Review: {text_batch.numpy()[i]}')\n",
    "        label = label_batch.numpy()[i]\n",
    "        print(f'Label : {label} ({class_names[label]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX8FtlpGJRE6"
   },
   "source": [
    "## Loading models from TensorFlow Hub\n",
    "\n",
    "Here you can choose which BERT model you will load from TensorFlow Hub and fine-tune. There are multiple BERT models available.\n",
    "\n",
    "  - [BERT-Base](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3), [Uncased](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3) and [seven more models](https://tfhub.dev/google/collections/bert/1) with trained weights released by the original BERT authors.\n",
    "  - [Small BERTs](https://tfhub.dev/google/collections/bert/1) have the same general architecture but fewer and/or smaller Transformer blocks, which lets you explore tradeoffs between speed, size and quality.\n",
    "  - [ALBERT](https://tfhub.dev/google/collections/albert/1): four different sizes of \"A Lite BERT\" that reduces model size (but not computation time) by sharing parameters between layers.\n",
    "  - [BERT Experts](https://tfhub.dev/google/collections/experts/bert/1): eight models that all have the BERT-base architecture but offer a choice between different pre-training domains, to align more closely with the target task.\n",
    "  - [Electra](https://tfhub.dev/google/collections/electra/1) has the same architecture as BERT (in three different sizes), but gets pre-trained as a discriminator in a set-up that resembles a Generative Adversarial Network (GAN).\n",
    "  - BERT with Talking-Heads Attention and Gated GELU [[base](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1), [large](https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1)] has two improvements to the core of the Transformer architecture.\n",
    "\n",
    "The model documentation on TensorFlow Hub has more details and references to the\n",
    "research literature. Follow the links above, or click on the [`tfhub.dev`](http://tfhub.dev) URL\n",
    "printed after the next cell execution.\n",
    "\n",
    "The suggestion is to start with a Small BERT (with fewer parameters) since they are faster to fine-tune. If you like a small model but with higher accuracy, ALBERT might be your next option. If you want even better accuracy, choose\n",
    "one of the classic BERT sizes or their recent refinements like Electra, Talking Heads, or a BERT Expert.\n",
    "\n",
    "Aside from the models available below, there are [multiple versions](https://tfhub.dev/google/collections/transformer_encoders_text/1) of the models that are larger and can yeld even better accuracy but they are too big to be fine-tuned on a single GPU. You will be able to do that on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu).\n",
    "\n",
    "You'll see in the code below that switching the tfhub.dev URL is enough to try any of these models, because all the differences between them are encapsulated in the SavedModels from TF Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:23.630238Z",
     "iopub.status.busy": "2020-11-26T02:27:23.629440Z",
     "iopub.status.idle": "2020-11-26T02:27:23.631872Z",
     "shell.execute_reply": "2020-11-26T02:27:23.632354Z"
    },
    "id": "y8_ctG55-uTX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1\n"
     ]
    }
   ],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/1',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WrcxxTRDdHi"
   },
   "source": [
    "## The preprocessing model\n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT. TensorFlow Hub provides a matching preprocessing model for each of the BERT models discussed above, which implements this transformation using TF ops from the TF.text library. It is not necessary to run pure Python code outside your TensorFlow model to preprocess text.\n",
    "\n",
    "The preprocessing model must be the one referenced by the documentation of the BERT model, which you can read at the URL printed above. For BERT models from the drop-down above, the preprocessing model is selected automatically.\n",
    "\n",
    "Note: You will load the preprocessing model into a [hub.KerasLayer](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to compose your fine-tuned model. This is the preferred API to load a TF2-style SavedModel from TF Hub into a Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:23.636633Z",
     "iopub.status.busy": "2020-11-26T02:27:23.635967Z",
     "iopub.status.idle": "2020-11-26T02:27:26.763082Z",
     "shell.execute_reply": "2020-11-26T02:27:26.762469Z"
    },
    "id": "0SQi-jWd_jzq"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4naBiEE_cZX"
   },
   "source": [
    "Let's try the preprocessing model on some text and see the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:26.774067Z",
     "iopub.status.busy": "2020-11-26T02:27:26.772558Z",
     "iopub.status.idle": "2020-11-26T02:27:27.252728Z",
     "shell.execute_reply": "2020-11-26T02:27:27.252173Z"
    },
    "id": "r9-zCzJpnuwS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_word_ids', 'input_type_ids', 'input_mask']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['this is such an amazing movie!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqL7ihkN_862"
   },
   "source": [
    "As you can see, now you have the 3 outputs from the preprocessing that a BERT model would use (`input_words_id`, `input_mask` and `input_type_ids`).\n",
    "\n",
    "Some other important points:\n",
    "- The input is truncated to 128 tokens. The number of tokens can be customized and you can see more details on the [Solve GLUE tasks using BERT on a TPU colab](https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu).\n",
    "- The `input_type_ids` only have one value (0) because this is a single sentence input. For a multiple sentence input, it would have one number for each input.\n",
    "\n",
    "Since this text preprocessor is a TensorFlow model, It can be included in your model directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DKnLPSEmtp9i"
   },
   "source": [
    "## Using the BERT model\n",
    "\n",
    "Before putting BERT into your own model, let's take a look at its outputs. You will load it from TF Hub and see the returned values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:27.258146Z",
     "iopub.status.busy": "2020-11-26T02:27:27.257474Z",
     "iopub.status.idle": "2020-11-26T02:27:39.731186Z",
     "shell.execute_reply": "2020-11-26T02:27:39.730586Z"
    },
    "id": "tXxYpK8ixL34"
   },
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:39.737569Z",
     "iopub.status.busy": "2020-11-26T02:27:39.736845Z",
     "iopub.status.idle": "2020-11-26T02:27:40.169604Z",
     "shell.execute_reply": "2020-11-26T02:27:40.170040Z"
    },
    "id": "_OoF9mebuSZc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.76262933  0.99280983 -0.18611841  0.36673838  0.1523371   0.65504414\n",
      "  0.9681154  -0.94862705  0.00216196 -0.9877732   0.06842704 -0.9763059 ]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[-0.28946352  0.3432127   0.33231536 ...  0.2130082   0.71020764\n",
      "  -0.05771179]\n",
      " [-0.28742087  0.31981015 -0.2301858  ...  0.5845512  -0.2132979\n",
      "   0.72692066]\n",
      " [-0.66156995  0.688768   -0.8743304  ...  0.10877246 -0.2617323\n",
      "   0.47855377]\n",
      " ...\n",
      " [-0.22561169 -0.2892561  -0.07064427 ...  0.47566015  0.8327705\n",
      "   0.4002538 ]\n",
      " [-0.29824227 -0.2747312  -0.05450515 ...  0.48849764  1.0955355\n",
      "   0.18163416]\n",
      " [-0.44378257  0.00930772  0.07223713 ...  0.17290124  1.1833248\n",
      "   0.07897964]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sm61jDrezAll"
   },
   "source": [
    "The BERT models return a map with 3 important keys: `pooled_output`, `sequence_output`, `encoder_outputs`:\n",
    "\n",
    "- `pooled_output` to represent each input sequence as a whole. The shape is `[batch_size, H]`. You can think of this as an embedding for the entire movie review.\n",
    "- `sequence_output` represents each input token in the context. The shape is `[batch_size, seq_length, H]`. You can think of this as a contextual embedding for every token in the movie review.\n",
    "- `encoder_outputs` are the intermediate activations of the `L` Transformer blocks. `outputs[\"encoder_outputs\"][i]` is a Tensor of shape `[batch_size, seq_length, 1024]` with the outputs of the i-th Transformer block, for `0 <= i < L`. The last value of the list is equal to `sequence_output`.\n",
    "\n",
    "For the fine-tuning you are going to use the `pooled_output` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDNKfAXbDnJH"
   },
   "source": [
    "## Define your model\n",
    "\n",
    "You will create a very simple fine-tuned model, with the preprocessing model, the selected BERT model, one Dense and a Dropout layer.\n",
    "\n",
    "Note: for more information about the base model's input and output you can use just follow the model's url for documentation. Here specifically you don't need to worry about it because the preprocessing model will take care of that for you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:40.176592Z",
     "iopub.status.busy": "2020-11-26T02:27:40.175958Z",
     "iopub.status.idle": "2020-11-26T02:27:40.177838Z",
     "shell.execute_reply": "2020-11-26T02:27:40.178261Z"
    },
    "id": "aksj743St9ga"
   },
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder_inputs = preprocessing_layer(text_input)\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=False, name='BERT_encoder')\n",
    "    outputs = encoder(encoder_inputs)\n",
    "    net = outputs['pooled_output']\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    #net =tf.keras.layers.Flatten()(net)\n",
    "    net = tf.keras.layers.Dense(1, activation='relu', name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zs4yhFraBuGQ"
   },
   "source": [
    "Let's check that the model runs with the output of the preprocessing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:40.184337Z",
     "iopub.status.busy": "2020-11-26T02:27:40.183278Z",
     "iopub.status.idle": "2020-11-26T02:27:48.551566Z",
     "shell.execute_reply": "2020-11-26T02:27:48.550999Z"
    },
    "id": "mGMF8AZcB2Zy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "text (InputLayer)               [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "preprocessing (KerasLayer)      {'input_type_ids': ( 0           text[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "BERT_encoder (KerasLayer)       {'pooled_output': (N 28763649    preprocessing[0][0]              \n",
      "                                                                 preprocessing[0][1]              \n",
      "                                                                 preprocessing[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 512)          0           BERT_encoder[0][5]               \n",
      "__________________________________________________________________________________________________\n",
      "classifier (Dense)              (None, 1)            513         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,764,162\n",
      "Trainable params: 513\n",
      "Non-trainable params: 28,763,649\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "#print(tf.sigmoid(bert_raw_result))\n",
    "\n",
    "classifier_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTUzNV2JE2G3"
   },
   "source": [
    "The output is meaningless, of course, because the model has not been trained yet.\n",
    "\n",
    "Let's take a look at the model's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:48.556641Z",
     "iopub.status.busy": "2020-11-26T02:27:48.555976Z",
     "iopub.status.idle": "2020-11-26T02:27:48.693541Z",
     "shell.execute_reply": "2020-11-26T02:27:48.694014Z"
    },
    "id": "0EmzyHZXKIpm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbUWoZMwc302"
   },
   "source": [
    "## Model training\n",
    "\n",
    "You now have all the pieces to train a model, including the preprocessing module, BERT encoder, data, and classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WpJ3xcwDT56v"
   },
   "source": [
    "### Loss function\n",
    "\n",
    "Since this is a binary classification problem and the model outputs a probability (a single-unit layer), you'll use `losses.BinaryCrossentropy` loss function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:48.708266Z",
     "iopub.status.busy": "2020-11-26T02:27:48.702894Z",
     "iopub.status.idle": "2020-11-26T02:27:48.713056Z",
     "shell.execute_reply": "2020-11-26T02:27:48.713470Z"
    },
    "id": "OWPOZE-L3AgE"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77psrpfzbxtp"
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "For fine-tuning, let's use the same optimizer that BERT was originally trained with: the \"Adaptive Moments\" (Adam). This optimizer minimizes the prediction loss and does regularization by weight decay (not using moments), which is also known as [AdamW](https://arxiv.org/abs/1711.05101).\n",
    "\n",
    "For the learning rate (`init_lr`), we use the same schedule as BERT pre-training: linear decay of a notional initial learning rate, prefixed with a linear warm-up phase over the first 10% of training steps (`num_warmup_steps`). In line with the BERT paper, the initial learning rate is smaller for fine-tuning (best of 5e-5, 3e-5, 2e-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.experimental.cardinality(train_ds).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:48.719248Z",
     "iopub.status.busy": "2020-11-26T02:27:48.718342Z",
     "iopub.status.idle": "2020-11-26T02:27:48.721639Z",
     "shell.execute_reply": "2020-11-26T02:27:48.721173Z"
    },
    "id": "P9eP2y9dbw32"
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqlarlpC_v0g"
   },
   "source": [
    "### Loading the BERT model and training\n",
    "\n",
    "Using the `classifier_model` you created earlier, you can compile the model with the loss, metric and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:48.734176Z",
     "iopub.status.busy": "2020-11-26T02:27:48.733432Z",
     "iopub.status.idle": "2020-11-26T02:27:48.743088Z",
     "shell.execute_reply": "2020-11-26T02:27:48.742471Z"
    },
    "id": "-7GPDhR98jsD"
   },
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpBuV5j2cS_b"
   },
   "source": [
    "Note: training time will vary depending on the complexity of the BERT model you have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:27:48.747703Z",
     "iopub.status.busy": "2020-11-26T02:27:48.747039Z",
     "iopub.status.idle": "2020-11-26T02:33:37.002672Z",
     "shell.execute_reply": "2020-11-26T02:33:37.002115Z"
    },
    "id": "HtfDFAnN_Neu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:235 call  *\n        result = smart_cond.smart_cond(training,\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:509 _call_attribute  **\n        return instance.__call__(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:780 __call__\n        result = self._call(*args, **kwds)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:814 _call\n        results = self._stateful_fn(*args, **kwds)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2828 __call__\n        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3213 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3065 _create_graph_function\n        func_graph_module.func_graph_from_py_func(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:600 wrapped_fn\n        return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py:251 restored_function_body\n        raise ValueError(\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None, None), dtype=string)\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * True\n        * None\n      Keyword arguments: {}\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-09a950764e7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m history = classifier_model.fit(x=train_ds,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                \u001b[0;31m#validation_data=val_ds,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    821\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 696\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    697\u001b[0m             *args, **kwds))\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2854\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2855\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2856\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3213\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3065\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    971\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    974\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    975\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:806 train_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:235 call  *\n        result = smart_cond.smart_cond(training,\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:509 _call_attribute  **\n        return instance.__call__(*args, **kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:780 __call__\n        result = self._call(*args, **kwds)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:814 _call\n        results = self._stateful_fn(*args, **kwds)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2828 __call__\n        graph_function, args, kwargs = self._maybe_define_function(args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3213 _maybe_define_function\n        graph_function = self._create_graph_function(args, kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py:3065 _create_graph_function\n        func_graph_module.func_graph_from_py_func(\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py:986 func_graph_from_py_func\n        func_outputs = python_func(*func_args, **func_kwargs)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:600 wrapped_fn\n        return weak_wrapped_fn().__wrapped__(*args, **kwds)\n    /opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/saved_model/function_deserialization.py:251 restored_function_body\n        raise ValueError(\n\n    ValueError: Could not find matching function to call loaded from the SavedModel. Got:\n      Positional arguments (3 total):\n        * Tensor(\"inputs:0\", shape=(None, None), dtype=string)\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Expected these arguments to match one of the following 4 option(s):\n    \n    Option 1:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 2:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * True\n        * None\n      Keyword arguments: {}\n    \n    Option 3:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='inputs')\n        * False\n        * None\n      Keyword arguments: {}\n    \n    Option 4:\n      Positional arguments (3 total):\n        * TensorSpec(shape=(None,), dtype=tf.string, name='sentences')\n        * True\n        * None\n      Keyword arguments: {}\n"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "batch_size = 64\n",
    "\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               batch_size=batch_size,\n",
    "                               #validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBthMlTSV8kn"
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Let's see how the model performs. Two values will be returned. Loss (a number which represents the error, lower values are better), and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:33:37.007666Z",
     "iopub.status.busy": "2020-11-26T02:33:37.007006Z",
     "iopub.status.idle": "2020-11-26T02:34:22.972637Z",
     "shell.execute_reply": "2020-11-26T02:34:22.972105Z"
    },
    "id": "slqB-urBV9sP"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uttWpgmSfzq9"
   },
   "source": [
    "### Plot the accuracy and loss over time\n",
    "\n",
    "Based on the `History` object returned by `model.fit()`. You can plot the training and validation loss for comparison, as well as the training and validation accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:34:22.996346Z",
     "iopub.status.busy": "2020-11-26T02:34:22.994489Z",
     "iopub.status.idle": "2020-11-26T02:34:23.283494Z",
     "shell.execute_reply": "2020-11-26T02:34:23.283976Z"
    },
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzJZCo-cf-Jf"
   },
   "source": [
    "In this plot, the red lines represents the training loss and accuracy, and the blue lines are the validation loss and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rtn7jewb6dg4"
   },
   "source": [
    "## Export for inference\n",
    "\n",
    "Now you just save your fine-tuned model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:34:23.292337Z",
     "iopub.status.busy": "2020-11-26T02:34:23.291622Z",
     "iopub.status.idle": "2020-11-26T02:34:29.832654Z",
     "shell.execute_reply": "2020-11-26T02:34:29.833169Z"
    },
    "id": "ShcvqJAgVera"
   },
   "outputs": [],
   "source": [
    "dataset_name = 'imdb'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbI25bS1vD7s"
   },
   "source": [
    "Let's reload the model so you can try it side by side with the model that is still in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:34:29.839338Z",
     "iopub.status.busy": "2020-11-26T02:34:29.838642Z",
     "iopub.status.idle": "2020-11-26T02:34:38.512999Z",
     "shell.execute_reply": "2020-11-26T02:34:38.513500Z"
    },
    "id": "gUEWVskZjEF0"
   },
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oyTappHTvNCz"
   },
   "source": [
    "Here you can test your model on any sentence you want, just add to the examples variable below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:34:38.523383Z",
     "iopub.status.busy": "2020-11-26T02:34:38.522193Z",
     "iopub.status.idle": "2020-11-26T02:34:39.788442Z",
     "shell.execute_reply": "2020-11-26T02:34:39.788861Z"
    },
    "id": "VBWzH6exlCPS"
   },
   "outputs": [],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
    "                         for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()\n",
    "\n",
    "\n",
    "examples = [\n",
    "    'this is such an amazing movie!',  # this is the same sentence tried earlier\n",
    "    'The movie was great!',\n",
    "    'The movie was meh.',\n",
    "    'The movie was okish.',\n",
    "    'The movie was terrible...'\n",
    "]\n",
    "\n",
    "reloaded_results = tf.sigmoid(reloaded_model(tf.constant(examples)))\n",
    "original_results = tf.sigmoid(classifier_model(tf.constant(examples)))\n",
    "\n",
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cOmih754Y_M"
   },
   "source": [
    "If you want to use your model on [TF Serving](https://www.tensorflow.org/tfx/guide/serving), remember that it will call your SavedModel through one of its named signatures. In Python, you can test them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-26T02:34:39.793456Z",
     "iopub.status.busy": "2020-11-26T02:34:39.792811Z",
     "iopub.status.idle": "2020-11-26T02:34:40.375593Z",
     "shell.execute_reply": "2020-11-26T02:34:40.376093Z"
    },
    "id": "0FdVD3973S-O"
   },
   "outputs": [],
   "source": [
    "serving_results = reloaded_model \\\n",
    "            .signatures['serving_default'](tf.constant(examples))\n",
    "\n",
    "serving_results = tf.sigmoid(serving_results['classifier'])\n",
    "\n",
    "print_my_examples(examples, serving_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4gN1KwReLPN"
   },
   "source": [
    "## Next steps\n",
    "\n",
    "As a next step, you can try [Solve GLUE tasks using BERT on a TPU tutorial](https://www.tensorflow.org/tutorials/text/solve_glue_tasks_using_bert_on_tpu) which runs on a TPU and shows you how to work with multiple inputs."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classify text with bert",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1qCzRlMTZm4nbt-UnvAjwLRKLmI9O9QAf",
     "timestamp": 1605566248897
    },
    {
     "file_id": "1sDc-4RmNbxsnX1h9YD9vjlODCVwgB_9u",
     "timestamp": 1605010177694
    },
    {
     "file_id": "1Td6ZBsdZ4nlKwb_Hzl8om9ZLoFqTQbQu",
     "timestamp": 1592639348558
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
